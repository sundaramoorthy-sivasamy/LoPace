{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LoPace: Complete User Guide\n",
        "\n",
        "**Lossless Optimized Prompt Accurate Compression Engine**\n",
        "\n",
        "This notebook provides a comprehensive guide to using LoPace for compressing and decompressing prompts with various configurations and use cases.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. [Introduction](#introduction)\n",
        "2. [Installation](#installation)\n",
        "3. [Quick Start](#quick-start)\n",
        "4. [Compression Methods](#compression-methods)\n",
        "   - Zstd Compression\n",
        "   - Token-based Compression\n",
        "   - Hybrid Compression (Recommended)\n",
        "5. [Configuration Options](#configuration-options)\n",
        "   - Tokenizer Models\n",
        "   - Zstd Compression Levels\n",
        "6. [Advanced Usage](#advanced-usage)\n",
        "   - Cross-Instance Compression/Decompression\n",
        "   - Batch Processing\n",
        "   - Compression Statistics\n",
        "7. [Real-World Examples](#real-world-examples)\n",
        "8. [Best Practices](#best-practices)\n",
        "9. [Performance Benchmarks](#performance-benchmarks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\n",
        "\n",
        "LoPace is a professional Python package for compressing and decompressing prompts using multiple lossless compression techniques. It's designed to help you:\n",
        "\n",
        "- **Reduce storage costs**: Achieve up to 80% space reduction\n",
        "- **Improve performance**: Fast compression/decompression speeds (50-200 MB/s)\n",
        "- **Maintain data integrity**: 100% lossless compression guarantees\n",
        "- **Scale efficiently**: Optimized for databases and large-scale applications\n",
        "\n",
        "### Key Features\n",
        "\n",
        "- âœ… **Three Compression Methods**: Zstd, Token-based (BPE), and Hybrid\n",
        "- âœ… **Lossless**: Perfect reconstruction of original prompts\n",
        "- âœ… **Configurable**: Multiple tokenizer models and compression levels\n",
        "- âœ… **Production-Ready**: Minimal memory footprint and excellent scalability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installation\n",
        "\n",
        "Install LoPace using pip:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install LoPace (if not already installed)\n",
        "# !pip install lopace\n",
        "\n",
        "# Import the required modules\n",
        "from lopace import PromptCompressor, CompressionMethod\n",
        "import sys\n",
        "\n",
        "print(f\"Python version: {sys.version}\")\n",
        "print(f\"LoPace imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quick Start\n",
        "\n",
        "The simplest way to use LoPace - compress and decompress a prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize compressor with default settings\n",
        "compressor = PromptCompressor()\n",
        "\n",
        "# Your prompt\n",
        "prompt = \"You are a helpful AI assistant designed to provide accurate and detailed responses.\"\n",
        "\n",
        "# Compress using hybrid method (recommended - best compression)\n",
        "compressed = compressor.compress(prompt, CompressionMethod.HYBRID)\n",
        "\n",
        "# Decompress back to original\n",
        "original = compressor.decompress(compressed, CompressionMethod.HYBRID)\n",
        "\n",
        "# Verify losslessness\n",
        "print(f\"Original: {prompt}\")\n",
        "print(f\"Decompressed: {original}\")\n",
        "print(f\"Match: {original == prompt} âœ“\")\n",
        "print(f\"\\nOriginal size: {len(prompt.encode('utf-8'))} bytes\")\n",
        "print(f\"Compressed size: {len(compressed)} bytes\")\n",
        "print(f\"Space saved: {(1 - len(compressed)/len(prompt.encode('utf-8')))*100:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compression Methods\n",
        "\n",
        "LoPace provides three compression methods, each with different characteristics:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Method 1: Zstd Compression\n",
        "\n",
        "Uses Zstandard's dictionary-based algorithm to find repeated patterns. Best for general text compression when tokenization overhead is not needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "compressor = PromptCompressor()\n",
        "\n",
        "prompt = \"\"\"You are an expert software engineer with expertise in Python, JavaScript, \n",
        "and cloud architecture. Provide detailed, well-structured code examples and explanations.\"\"\"\n",
        "\n",
        "# Zstd compression\n",
        "zstd_compressed = compressor.compress_zstd(prompt)\n",
        "zstd_decompressed = compressor.decompress_zstd(zstd_compressed)\n",
        "\n",
        "print(\"=== Zstd Compression ===\")\n",
        "print(f\"Original size: {len(prompt.encode('utf-8'))} bytes\")\n",
        "print(f\"Compressed size: {len(zstd_compressed)} bytes\")\n",
        "print(f\"Compression ratio: {len(prompt.encode('utf-8'))/len(zstd_compressed):.2f}x\")\n",
        "print(f\"Space saved: {(1 - len(zstd_compressed)/len(prompt.encode('utf-8')))*100:.1f}%\")\n",
        "print(f\"Lossless: {zstd_decompressed == prompt} âœ“\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Method 2: Token-based Compression\n",
        "\n",
        "Uses Byte-Pair Encoding (BPE) to convert text to token IDs, then packs them as binary data. Best when you need token IDs anyway or are working with LLM tokenizers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Token-based compression\n",
        "token_compressed = compressor.compress_token(prompt)\n",
        "token_decompressed = compressor.decompress_token(token_compressed)\n",
        "\n",
        "print(\"=== Token-based Compression ===\")\n",
        "print(f\"Original size: {len(prompt.encode('utf-8'))} bytes\")\n",
        "print(f\"Compressed size: {len(token_compressed)} bytes\")\n",
        "print(f\"Compression ratio: {len(prompt.encode('utf-8'))/len(token_compressed):.2f}x\")\n",
        "print(f\"Space saved: {(1 - len(token_compressed)/len(prompt.encode('utf-8')))*100:.1f}%\")\n",
        "print(f\"Lossless: {token_decompressed == prompt} âœ“\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Method 3: Hybrid Compression (Recommended)\n",
        "\n",
        "Combines tokenization and Zstd compression for maximum efficiency. This is the **recommended method** for database storage where maximum compression is needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hybrid compression (best compression ratio)\n",
        "hybrid_compressed = compressor.compress_hybrid(prompt)\n",
        "hybrid_decompressed = compressor.decompress_hybrid(hybrid_compressed)\n",
        "\n",
        "print(\"=== Hybrid Compression (Recommended) ===\")\n",
        "print(f\"Original size: {len(prompt.encode('utf-8'))} bytes\")\n",
        "print(f\"Compressed size: {len(hybrid_compressed)} bytes\")\n",
        "print(f\"Compression ratio: {len(prompt.encode('utf-8'))/len(hybrid_compressed):.2f}x\")\n",
        "print(f\"Space saved: {(1 - len(hybrid_compressed)/len(prompt.encode('utf-8')))*100:.1f}%\")\n",
        "print(f\"Lossless: {hybrid_decompressed == prompt} âœ“\")\n",
        "\n",
        "# Compare all methods\n",
        "print(\"\\n=== Comparison of All Methods ===\")\n",
        "methods = {\n",
        "    \"Zstd\": (zstd_compressed, zstd_decompressed),\n",
        "    \"Token\": (token_compressed, token_decompressed),\n",
        "    \"Hybrid\": (hybrid_compressed, hybrid_decompressed)\n",
        "}\n",
        "\n",
        "for method_name, (compressed, decompressed) in methods.items():\n",
        "    ratio = len(prompt.encode('utf-8'))/len(compressed)\n",
        "    saved = (1 - len(compressed)/len(prompt.encode('utf-8')))*100\n",
        "    print(f\"{method_name:8s}: {len(compressed):4d} bytes, {ratio:.2f}x ratio, {saved:5.1f}% saved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration Options\n",
        "\n",
        "LoPace provides several configuration options to optimize compression for your specific use case:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tokenizer Models\n",
        "\n",
        "Different tokenizer models can be used depending on your needs. The default is `cl100k_base` (OpenAI's GPT-4 tokenizer)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = \"The quick brown fox jumps over the lazy dog. ðŸ¦Š\"\n",
        "\n",
        "# Test different tokenizer models\n",
        "models = [\"cl100k_base\", \"p50k_base\", \"r50k_base\"]\n",
        "\n",
        "print(\"=== Comparing Tokenizer Models ===\")\n",
        "print(f\"Prompt: {prompt}\\n\")\n",
        "\n",
        "for model in models:\n",
        "    try:\n",
        "        compressor = PromptCompressor(model=model)\n",
        "        \n",
        "        # Get token count\n",
        "        tokens = compressor.tokenizer.encode(prompt)\n",
        "        \n",
        "        # Compress using hybrid method\n",
        "        compressed = compressor.compress_hybrid(prompt)\n",
        "        \n",
        "        print(f\"\\nModel: {model}\")\n",
        "        print(f\"  Token count: {len(tokens)}\")\n",
        "        print(f\"  Original size: {len(prompt.encode('utf-8'))} bytes\")\n",
        "        print(f\"  Compressed size: {len(compressed)} bytes\")\n",
        "        print(f\"  Space saved: {(1 - len(compressed)/len(prompt.encode('utf-8')))*100:.1f}%\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nModel: {model} - Error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Zstd Compression Levels\n",
        "\n",
        "Zstd compression levels range from 1 (fastest, less compression) to 22 (slowest, best compression). The default is 15 (balanced)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "long_prompt = \"\"\"You are a comprehensive AI assistant specializing in technical documentation \n",
        "and educational content. Your expertise spans multiple domains including computer science, \n",
        "data science, machine learning, software engineering, and web development. When responding \n",
        "to queries, you should provide thorough explanations, include relevant examples, and \n",
        "structure your responses in a clear and organized manner.\"\"\" * 5\n",
        "\n",
        "print(\"=== Comparing Zstd Compression Levels ===\")\n",
        "print(f\"Prompt length: {len(long_prompt)} characters\\n\")\n",
        "\n",
        "# Test different compression levels\n",
        "levels = [1, 5, 10, 15, 19, 22]\n",
        "\n",
        "results = []\n",
        "for level in levels:\n",
        "    compressor = PromptCompressor(zstd_level=level)\n",
        "    \n",
        "    # Time the compression\n",
        "    start = time.perf_counter()\n",
        "    compressed = compressor.compress_hybrid(long_prompt)\n",
        "    compression_time = (time.perf_counter() - start) * 1000  # ms\n",
        "    \n",
        "    original_size = len(long_prompt.encode('utf-8'))\n",
        "    compressed_size = len(compressed)\n",
        "    space_saved = (1 - compressed_size/original_size) * 100\n",
        "    \n",
        "    results.append({\n",
        "        'level': level,\n",
        "        'size': compressed_size,\n",
        "        'time': compression_time,\n",
        "        'saved': space_saved\n",
        "    })\n",
        "    \n",
        "    print(f\"Level {level:2d}: {compressed_size:5d} bytes, \"\n",
        "          f\"{space_saved:5.1f}% saved, {compression_time:6.2f} ms\")\n",
        "\n",
        "print(\"\\nðŸ’¡ Tip: Higher levels (15-19) provide best balance of compression and speed.\")\n",
        "print(\"      Level 22 maximizes compression but can be significantly slower.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced Usage\n",
        "\n",
        "### Cross-Instance Compression/Decompression\n",
        "\n",
        "**Important**: You can compress with one instance and decompress with another, as long as you use the **same tokenizer model**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = \"This is a test prompt for cross-instance compression.\"\n",
        "\n",
        "# Compress with one instance\n",
        "compressor1 = PromptCompressor(model=\"cl100k_base\", zstd_level=15)\n",
        "compressed = compressor1.compress_hybrid(prompt)\n",
        "\n",
        "# Decompress with a NEW instance (same model)\n",
        "compressor2 = PromptCompressor(model=\"cl100k_base\", zstd_level=20)  # Different zstd_level is OK\n",
        "original = compressor2.decompress_hybrid(compressed)\n",
        "\n",
        "print(\"=== Cross-Instance Compression/Decompression ===\")\n",
        "print(f\"Original: {prompt}\")\n",
        "print(f\"Decompressed: {original}\")\n",
        "print(f\"Match: {original == prompt} âœ“\")\n",
        "print(\"\\nâœ… Works perfectly as long as both instances use the same tokenizer model!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Batch Processing\n",
        "\n",
        "Compress and decompress multiple prompts efficiently:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "compressor = PromptCompressor()\n",
        "\n",
        "# Batch of prompts\n",
        "prompts = [\n",
        "    \"You are a helpful AI assistant.\",\n",
        "    \"Translate the following text to French.\",\n",
        "    \"Summarize this document in 3 sentences.\",\n",
        "    \"You are an expert Python developer.\",\n",
        "    \"Explain the concept of machine learning.\"\n",
        "]\n",
        "\n",
        "print(\"=== Batch Processing ===\")\n",
        "print(f\"Processing {len(prompts)} prompts...\\n\")\n",
        "\n",
        "compressed_batch = []\n",
        "decompressed_batch = []\n",
        "\n",
        "for i, prompt in enumerate(prompts, 1):\n",
        "    # Compress\n",
        "    compressed = compressor.compress_hybrid(prompt)\n",
        "    compressed_batch.append(compressed)\n",
        "    \n",
        "    # Decompress\n",
        "    decompressed = compressor.decompress_hybrid(compressed)\n",
        "    decompressed_batch.append(decompressed)\n",
        "    \n",
        "    # Verify\n",
        "    original_size = len(prompt.encode('utf-8'))\n",
        "    compressed_size = len(compressed)\n",
        "    space_saved = (1 - compressed_size/original_size) * 100\n",
        "    \n",
        "    print(f\"Prompt {i}: {original_size:3d} â†’ {compressed_size:3d} bytes \"\n",
        "          f\"({space_saved:5.1f}% saved), Lossless: {decompressed == prompt} âœ“\")\n",
        "\n",
        "# Verify all\n",
        "all_match = all(orig == decomp for orig, decomp in zip(prompts, decompressed_batch))\n",
        "print(f\"\\nâœ… All prompts processed successfully: {all_match}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compression Statistics\n",
        "\n",
        "Get detailed statistics about compression performance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "compressor = PromptCompressor()\n",
        "\n",
        "prompt = \"\"\"You are a comprehensive AI assistant specializing in technical documentation \n",
        "and educational content. Your expertise spans multiple domains including computer science, \n",
        "data science, machine learning, software engineering, and web development.\"\"\"\n",
        "\n",
        "# Get statistics for all methods\n",
        "stats = compressor.get_compression_stats(prompt)\n",
        "\n",
        "print(\"=== Compression Statistics ===\")\n",
        "print(f\"Original Size: {stats['original_size_bytes']} bytes\")\n",
        "print(f\"Original Tokens: {stats['original_size_tokens']}\")\n",
        "print(\"\\nMethod Comparison:\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for method, method_stats in stats['methods'].items():\n",
        "    print(f\"\\n{method.upper()}:\")\n",
        "    print(f\"  Compressed Size: {method_stats['compressed_size_bytes']} bytes\")\n",
        "    print(f\"  Compression Ratio: {method_stats['compression_ratio']:.2f}x\")\n",
        "    print(f\"  Space Saved: {method_stats['space_saved_percent']:.2f}%\")\n",
        "    print(f\"  Bytes Saved: {method_stats['bytes_saved']} bytes\")\n",
        "\n",
        "# Get stats for a specific method\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Statistics for Hybrid method only:\")\n",
        "hybrid_stats = compressor.get_compression_stats(prompt, CompressionMethod.HYBRID)\n",
        "print(f\"Compressed Size: {hybrid_stats['methods']['hybrid']['compressed_size_bytes']} bytes\")\n",
        "print(f\"Space Saved: {hybrid_stats['methods']['hybrid']['space_saved_percent']:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using `compress_and_return_both`\n",
        "\n",
        "A convenience method that returns both the original and compressed versions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "compressor = PromptCompressor()\n",
        "\n",
        "prompt = \"You are a helpful AI assistant.\"\n",
        "\n",
        "# Get both original and compressed\n",
        "original, compressed = compressor.compress_and_return_both(prompt, CompressionMethod.HYBRID)\n",
        "\n",
        "print(\"=== Using compress_and_return_both ===\")\n",
        "print(f\"Original: {original}\")\n",
        "print(f\"Original stored: {original == prompt} âœ“\")\n",
        "print(f\"Compressed size: {len(compressed)} bytes\")\n",
        "print(f\"Original size: {len(prompt.encode('utf-8'))} bytes\")\n",
        "print(f\"Space saved: {(1 - len(compressed)/len(prompt.encode('utf-8')))*100:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Real-World Examples\n",
        "\n",
        "### Example 1: Storing System Prompts in a Database\n",
        "\n",
        "Save space when storing system prompts for multiple users:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "compressor = PromptCompressor()\n",
        "\n",
        "# System prompt template\n",
        "system_prompt = \"\"\"You are a professional customer service AI assistant. \n",
        "Your role is to help customers with their inquiries in a friendly, helpful, and efficient manner. \n",
        "Always be polite, professional, and aim to resolve issues quickly. If you cannot help, \n",
        "escalate to a human agent.\"\"\"\n",
        "\n",
        "# Simulate storing for multiple users\n",
        "num_users = 1000\n",
        "original_total = len(system_prompt.encode('utf-8')) * num_users\n",
        "\n",
        "# Compress once and reuse\n",
        "compressed_prompt = compressor.compress_hybrid(system_prompt)\n",
        "compressed_total = len(compressed_prompt) * num_users\n",
        "\n",
        "print(\"=== Database Storage Example ===\")\n",
        "print(f\"System prompt size: {len(system_prompt.encode('utf-8'))} bytes\")\n",
        "print(f\"Compressed size: {len(compressed_prompt)} bytes\")\n",
        "print(f\"\\nFor {num_users:,} users:\")\n",
        "print(f\"  Original total: {original_total:,} bytes ({original_total/1024/1024:.2f} MB)\")\n",
        "print(f\"  Compressed total: {compressed_total:,} bytes ({compressed_total/1024/1024:.2f} MB)\")\n",
        "print(f\"  Space saved: {(1 - compressed_total/original_total)*100:.1f}%\")\n",
        "print(f\"  Savings: {(original_total - compressed_total)/1024/1024:.2f} MB\")\n",
        "\n",
        "# Verify we can decompress\n",
        "decompressed = compressor.decompress_hybrid(compressed_prompt)\n",
        "print(f\"\\nâœ… Lossless: {decompressed == system_prompt}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 2: Compressing Conversation Histories\n",
        "\n",
        "Store conversation histories efficiently:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "compressor = PromptCompressor()\n",
        "\n",
        "# Simulate a conversation history\n",
        "conversation = \"\"\"User: What is Python?\n",
        "Assistant: Python is a high-level programming language known for its simplicity and readability.\n",
        "User: Can you give me an example?\n",
        "Assistant: Sure! Here's a simple example:\n",
        "```python\n",
        "def greet(name):\n",
        "    return f\"Hello, {name}!\"\n",
        "print(greet(\"World\"))\n",
        "```\n",
        "User: Thanks!\n",
        "Assistant: You're welcome! Let me know if you need any other help.\"\"\"\n",
        "\n",
        "# Compress conversation\n",
        "compressed_conv = compressor.compress_hybrid(conversation)\n",
        "decompressed_conv = compressor.decompress_hybrid(compressed_conv)\n",
        "\n",
        "print(\"=== Conversation History Compression ===\")\n",
        "print(f\"Original size: {len(conversation.encode('utf-8'))} bytes\")\n",
        "print(f\"Compressed size: {len(compressed_conv)} bytes\")\n",
        "print(f\"Space saved: {(1 - len(compressed_conv)/len(conversation.encode('utf-8')))*100:.1f}%\")\n",
        "print(f\"Lossless: {decompressed_conv == conversation} âœ“\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 3: Compressing LLM API Responses\n",
        "\n",
        "Reduce storage when caching LLM responses:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "compressor = PromptCompressor()\n",
        "\n",
        "# Simulate LLM response\n",
        "llm_response = \"\"\"Machine Learning (ML) is a subset of artificial intelligence that enables \n",
        "systems to learn and improve from experience without being explicitly programmed. It focuses \n",
        "on developing algorithms that can access data and use it to learn patterns and make predictions \n",
        "or decisions. There are three main types of machine learning:\n",
        "1. Supervised Learning: Uses labeled data to train models\n",
        "2. Unsupervised Learning: Finds patterns in unlabeled data\n",
        "3. Reinforcement Learning: Learns through trial and error with rewards/penalties\"\"\"\n",
        "\n",
        "# Compress response\n",
        "compressed_response = compressor.compress_hybrid(llm_response)\n",
        "decompressed_response = compressor.decompress_hybrid(compressed_response)\n",
        "\n",
        "print(\"=== API Response Compression ===\")\n",
        "print(f\"Original size: {len(llm_response.encode('utf-8'))} bytes\")\n",
        "print(f\"Compressed size: {len(compressed_response)} bytes\")\n",
        "print(f\"Compression ratio: {len(llm_response.encode('utf-8'))/len(compressed_response):.2f}x\")\n",
        "print(f\"Space saved: {(1 - len(compressed_response)/len(llm_response.encode('utf-8')))*100:.1f}%\")\n",
        "print(f\"Lossless: {decompressed_response == llm_response} âœ“\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Best Practices\n",
        "\n",
        "### 1. Choose the Right Method\n",
        "\n",
        "- **Hybrid** (Recommended): Best compression ratio, ideal for database storage\n",
        "- **Token**: Use when you need token IDs anyway or working with LLM tokenizers\n",
        "- **Zstd**: Fast and simple, good for general text compression\n",
        "\n",
        "### 2. Select Appropriate Configuration\n",
        "\n",
        "- **Tokenizer Model**: Use `cl100k_base` (default) for GPT-4 compatibility, or choose based on your LLM\n",
        "- **Zstd Level**: Use 15 (default) for balanced performance, or adjust based on your speed/storage priorities\n",
        "\n",
        "### 3. Maintain Consistency\n",
        "\n",
        "- Use the same tokenizer model for compression and decompression\n",
        "- Document your compression settings for reproducibility\n",
        "\n",
        "### 4. Consider Your Use Case\n",
        "\n",
        "- **Large-scale storage**: Use Hybrid method with zstd_level 15-19\n",
        "- **Fast processing needed**: Use lower zstd_level (5-10) or Zstd method\n",
        "- **Real-time applications**: Consider lower compression levels for speed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Performance Benchmarks\n",
        "\n",
        "Let's benchmark compression performance across different prompt sizes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "compressor = PromptCompressor()\n",
        "\n",
        "# Test prompts of different sizes\n",
        "test_prompts = {\n",
        "    \"Small\": \"You are a helpful AI assistant.\",\n",
        "    \"Medium\": \"\"\"You are a helpful AI assistant designed to provide accurate, \n",
        "detailed, and helpful responses to user queries. Your goal is to assist users \n",
        "by understanding their questions and providing relevant information.\"\"\",\n",
        "    \"Large\": \"\"\"You are a comprehensive AI assistant specializing in technical documentation \n",
        "and educational content. Your expertise spans multiple domains including computer science, \n",
        "data science, machine learning, software engineering, and web development. When responding \n",
        "to queries, you should provide thorough explanations, include relevant examples, and \n",
        "structure your responses in a clear and organized manner. Always aim to educate while \n",
        "solving problems. Break down complex concepts into digestible parts, use analogies when \n",
        "helpful, and provide practical applications of theoretical knowledge.\"\"\" * 3\n",
        "}\n",
        "\n",
        "print(\"=== Performance Benchmarks ===\")\n",
        "print(\"Testing all three compression methods:\\n\")\n",
        "\n",
        "results = []\n",
        "\n",
        "for size_name, prompt in test_prompts.items():\n",
        "    original_size = len(prompt.encode('utf-8'))\n",
        "    \n",
        "    print(f\"\\n{size_name} Prompt ({original_size} bytes):\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    for method in [CompressionMethod.ZSTD, CompressionMethod.TOKEN, CompressionMethod.HYBRID]:\n",
        "        # Time compression\n",
        "        start = time.perf_counter()\n",
        "        compressed = compressor.compress(prompt, method)\n",
        "        compression_time = (time.perf_counter() - start) * 1000  # ms\n",
        "        \n",
        "        # Time decompression\n",
        "        start = time.perf_counter()\n",
        "        decompressed = compressor.decompress(compressed, method)\n",
        "        decompression_time = (time.perf_counter() - start) * 1000  # ms\n",
        "        \n",
        "        # Verify losslessness\n",
        "        is_lossless = decompressed == prompt\n",
        "        \n",
        "        compressed_size = len(compressed)\n",
        "        space_saved = (1 - compressed_size/original_size) * 100\n",
        "        \n",
        "        results.append({\n",
        "            'size': size_name,\n",
        "            'method': method.value,\n",
        "            'original': original_size,\n",
        "            'compressed': compressed_size,\n",
        "            'space_saved': space_saved,\n",
        "            'compression_time': compression_time,\n",
        "            'decompression_time': decompression_time,\n",
        "            'lossless': is_lossless\n",
        "        })\n",
        "        \n",
        "        print(f\"{method.value:8s}: {compressed_size:4d} bytes, \"\n",
        "              f\"{space_saved:5.1f}% saved, \"\n",
        "              f\"compress: {compression_time:5.2f}ms, \"\n",
        "              f\"decompress: {decompression_time:5.2f}ms, \"\n",
        "              f\"Lossless: {'âœ“' if is_lossless else 'âœ—'}\")\n",
        "\n",
        "print(\"\\nâœ… All benchmarks completed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "LoPace provides powerful, lossless compression for prompts with:\n",
        "\n",
        "- âœ… **Three compression methods** to choose from\n",
        "- âœ… **Flexible configuration** options (tokenizer models, compression levels)\n",
        "- âœ… **100% lossless** guarantees\n",
        "- âœ… **Excellent compression ratios** (up to 80% space savings)\n",
        "- âœ… **Production-ready** performance\n",
        "\n",
        "### Quick Reference\n",
        "\n",
        "```python\n",
        "from lopace import PromptCompressor, CompressionMethod\n",
        "\n",
        "# Initialize\n",
        "compressor = PromptCompressor(model=\"cl100k_base\", zstd_level=15)\n",
        "\n",
        "# Compress (Hybrid recommended)\n",
        "compressed = compressor.compress_hybrid(\"Your prompt here\")\n",
        "\n",
        "# Decompress\n",
        "original = compressor.decompress_hybrid(compressed)\n",
        "\n",
        "# Verify\n",
        "assert original == \"Your prompt here\"  # âœ“ Always True (lossless)\n",
        "```\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Check out the [LoPace documentation](https://github.com/amanulla/lopace)\n",
        "- Run the Streamlit app: `streamlit run streamlit_app.py`\n",
        "- Explore the source code for advanced use cases\n",
        "\n",
        "---\n",
        "\n",
        "**Happy Compressing! ðŸš€**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
